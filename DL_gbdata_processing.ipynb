{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea7f3fb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17724/3876138832.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstatistics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import segyio\n",
    "import numpy as np\n",
    "import glob\n",
    "import statistics\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84632e54",
   "metadata": {},
   "source": [
    "# Training data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a1660d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = r\"D:/Earth and Energy Sciences class/Spring'2022/CSCE 588- Neural Networks/CNN project/All data/Fault_detection_gigabyte_data/seismic_train\"\n",
    "path2 = r\"D:/Earth and Energy Sciences class/Spring'2022/CSCE 588- Neural Networks/CNN project/All data/Fault_detection_gigabyte_data/fault_train\"\n",
    "all_seis = sorted(glob.glob(path1+\"/*.npy\"))\n",
    "all_faults = sorted(glob.glob(path2+\"/*.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c65125c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"D:/Earth and Energy Sciences class/Spring'2022/CSCE 588- Neural Networks/CNN project/All data/Fault_detection_gigabyte_data/seismic_train\\\\seistrain6.npy\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_seis[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b64628d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"D:/Earth and Energy Sciences class/Spring'2022/CSCE 588- Neural Networks/CNN project/All data/Fault_detection_gigabyte_data/fault_train\\\\faulttrain5.npy\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_faults[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88368d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2400, 512, 512)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#seismic data with new shape: images tranformed into 1024*512 shape\n",
    "train_data = np.empty((0,512,512),dtype='float32')\n",
    "for filename in all_seis:\n",
    "    seis = np.load(filename)\n",
    "    seis_ns = seis[:,:2048,750:1262]          \n",
    "    seis_ns1, seis_ns2, seis_ns3, seis_ns4 = np.split(seis_ns, 4, axis = 1)\n",
    "    seis_ns_final = np.concatenate((seis_ns1,seis_ns2,seis_ns3,seis_ns4),axis=0)\n",
    "    train_data = np.append(train_data,seis_ns_final,axis=0)\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead6a2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "plt.imshow(train_data[0,:,:].T,cmap='gray',  vmin=0, vmax=0.4, alpha = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c75d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fault data with new shape: images tranformed into 512*512 shape\n",
    "fault_data = np.empty((0,512,512),dtype='float32')\n",
    "for filename in all_faults:\n",
    "    fault = np.load(filename)\n",
    "    fault_ns = fault[:,:2048,750:1262]          \n",
    "    fault_ns1, fault_ns2, fault_ns3, fault_ns4 = np.split(fault_ns, 4, axis = 1)\n",
    "    fault_ns_final = np.concatenate((fault_ns1,fault_ns2,fault_ns3,fault_ns4),axis=0)\n",
    "    fault_data = np.append(fault_data,fault_ns_final,axis=0)\n",
    "fault_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c321bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.025 * 512 * 512\n",
    "fault_data_refined = np.empty((0,512,512),dtype='float32')\n",
    "train_data_refined = np.empty((0,512,512),dtype='float32')\n",
    "for i in range(2400):\n",
    "    if np.count_nonzero(fault_data[i] == 1) > threshold:\n",
    "        fault_data_refined=np.append(fault_data_refined,fault_data[i:i+1],axis=0)\n",
    "        train_data_refined=np.append(train_data_refined,train_data[i:i+1],axis=0)\n",
    "fault_data_refined.shape , train_data_refined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224b8514",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seis_img = torch.from_numpy(train_data_refined).to(torch.float32)\n",
    "print(train_seis_img.shape)\n",
    "print(train_seis_img.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2813c158",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "plt.imshow(train_seis_img[0,:,:].T,cmap='gray',  vmin=0, vmax=0.4, alpha = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f159f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fault_img = torch.from_numpy(fault_data_refined).to(torch.float32)\n",
    "print(train_fault_img.shape)\n",
    "print(train_fault_img.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29569c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seis_list = []\n",
    "train_fault_list = []\n",
    "for i in range(1512):\n",
    "    train_seis_list.append(train_seis_img[i].unsqueeze(0))\n",
    "    train_fault_list.append(train_fault_img[i].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0499669",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seis_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a15cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data into Pytorch Dataset Utitlity\n",
    "train_volumes = torch.stack(train_seis_list)\n",
    "train_labels = torch.stack(train_fault_list)\n",
    "train_dataset = torch.utils.data.TensorDataset(train_volumes, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70075419",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_volumes.shape, train_labels.shape, train_volumes.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225ac86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One last sanity check\n",
    "fig = plt.figure(figsize=(40,40))\n",
    "    \n",
    "ax = fig.add_subplot(331)    \n",
    "plt.imshow(train_volumes[0].T, cmap=\"gray\");\n",
    "ax.set_title(\"Seismic Image\")\n",
    "    \n",
    "ax = fig.add_subplot(332)\n",
    "ax.imshow(train_volumes[0].T, cmap = 'gray')\n",
    "ax.imshow(train_labels[0].T, cmap = 'gray', vmin=0, vmax=1, alpha=0.3)\n",
    "ax.set_title(\"Fault\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24850ae",
   "metadata": {},
   "source": [
    "# Validation data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d77a52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path3 = r\"D:/Earth and Energy Sciences class/Spring'2022/CSCE 588- Neural Networks/CNN project/All data/Fault_detection_gigabyte_data/seismic_validation\"\n",
    "path4 = r\"D:/Earth and Energy Sciences class/Spring'2022/CSCE 588- Neural Networks/CNN project/All data/Fault_detection_gigabyte_data/fault_validation\"\n",
    "valid_seis = sorted(glob.glob(path3+\"/*.npy\"))\n",
    "valid_faults = sorted(glob.glob(path4+\"/*.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5e33bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_seis[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0659cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_faults[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed4b6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fault data with new shape: images tranformed into 512*512 shape\n",
    "valid_seis_data = np.empty((0,512,512),dtype='float32')\n",
    "for filename in valid_seis:\n",
    "    seis_v = np.load(filename)\n",
    "    seis_ns = seis_v[:,250:2298,800:1312]          \n",
    "    seis_ns1, seis_ns2, seis_ns3, seis_ns4 = np.split(seis_ns, 4, axis = 1)\n",
    "    seis_ns_final = np.concatenate((seis_ns1,seis_ns2,seis_ns3,seis_ns4),axis=0)\n",
    "    valid_seis_data = np.append(valid_seis_data,seis_ns_final,axis=0)\n",
    "valid_seis_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a51eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fault data with new shape: images tranformed into 512*512 shape\n",
    "valid_fault_data = np.empty((0,512,512),dtype='float32')\n",
    "for filename in valid_faults:\n",
    "    fault_v = np.load(filename)\n",
    "    fault_ns = fault_v[:,250:2298,800:1312]          \n",
    "    fault_ns1, fault_ns2, fault_ns3, fault_ns4 = np.split(fault_ns, 4, axis = 1)\n",
    "    fault_ns_final = np.concatenate((fault_ns1,fault_ns2,fault_ns3,fault_ns4),axis=0)\n",
    "    valid_fault_data = np.append(valid_fault_data,fault_ns_final,axis=0)\n",
    "valid_fault_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe14cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.025 * 512 * 512\n",
    "valid_fault_data_refined = np.empty((0,512,512),dtype='float32')\n",
    "valid_train_data_refined = np.empty((0,512,512),dtype='float32')\n",
    "for i in range(400):\n",
    "    if np.count_nonzero(valid_fault_data[i] == 1) > threshold:\n",
    "        valid_fault_data_refined=np.append(valid_fault_data_refined,valid_fault_data[i:i+1],axis=0)\n",
    "        valid_train_data_refined=np.append(valid_train_data_refined,valid_seis_data[i:i+1],axis=0)\n",
    "valid_fault_data_refined.shape , valid_train_data_refined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824eda4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_seis_img = torch.from_numpy(valid_train_data_refined).to(torch.float32)\n",
    "print(valid_seis_img.shape)\n",
    "print(valid_seis_img.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d6b94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_fault_img = torch.from_numpy(valid_fault_data_refined).to(torch.float32)\n",
    "print(valid_fault_img.shape)\n",
    "print(valid_fault_img.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88181461",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_seis_list = []\n",
    "valid_fault_list = []\n",
    "for i in range(400):\n",
    "    valid_seis_list.append(valid_seis_img[i].unsqueeze(0))\n",
    "    valid_fault_list.append(valid_fault_img[i].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f2c273",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_seis_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8eee3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data into Pytorch Dataset Utitlity\n",
    "valid_volumes = torch.stack(valid_seis_list)\n",
    "valid_labels = torch.stack(valid_fault_list)\n",
    "valid_dataset = torch.utils.data.TensorDataset(valid_volumes, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6ee91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_volumes.shape, valid_labels.shape, valid_volumes.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7ffbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One last sanity check\n",
    "fig = plt.figure(figsize=(40,40))\n",
    "    \n",
    "ax = fig.add_subplot(331)    \n",
    "plt.imshow(valid_volumes[0].T, cmap=\"gray\");\n",
    "ax.set_title(\"Seismic Image\")\n",
    "    \n",
    "ax = fig.add_subplot(332)\n",
    "ax.imshow(valid_volumes[0].T, cmap = 'gray')\n",
    "ax.imshow(valid_labels[0].T, cmap = 'gray', vmin=0, vmax=1, alpha=0.3)\n",
    "ax.set_title(\"Fault\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5142d4",
   "metadata": {},
   "source": [
    "# Build the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094f0545",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContractingBlock(nn.Module):\n",
    "    '''\n",
    "    ContractingBlock Class\n",
    "    Performs two convolutions followed by a max pool operation.\n",
    "    Values:\n",
    "        input_channels: the number of channels to expect from a given input\n",
    "    '''\n",
    "    def __init__(self, input_channels):\n",
    "        super(ContractingBlock, self).__init__()\n",
    "        \n",
    "        # double the number of channels in the first convolution\n",
    "        # and keep the same number of channels in the second.\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(input_channels, 2*input_channels, kernel_size=3, padding=(1,1))\n",
    "        self.conv2 = nn.Conv2d(2*input_channels, 2*input_channels, kernel_size=3, padding=(1,1))\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Function for completing a forward pass of ContractingBlock: \n",
    "        Given an image tensor, completes a contracting block and returns the transformed tensor.\n",
    "        Parameters:\n",
    "            x: image tensor of shape (batch size, channels, height, width)\n",
    "        '''\n",
    "        x = self.conv1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.maxpool(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fc364f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpandingBlock(nn.Module):\n",
    "    '''\n",
    "    ExpandingBlock Class\n",
    "    Performs an upsampling, a convolution, a concatenation of its two inputs,\n",
    "    followed by two more convolutions.\n",
    "    Values:\n",
    "        input_channels: the number of channels to expect from a given input\n",
    "    '''\n",
    "    def __init__(self, input_channels):\n",
    "        super(ExpandingBlock, self).__init__()\n",
    "        \n",
    "        # \"Every step in the expanding path consists of an upsampling of the feature map\"\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)      \n",
    "        self.conv1 = nn.Conv2d(input_channels, input_channels//2, kernel_size=3, padding=(1,1))\n",
    "        self.conv2 = nn.Conv2d(input_channels, input_channels//2, kernel_size=3, padding=(1,1))\n",
    "        self.conv3 = nn.Conv2d(input_channels//2, input_channels//2, kernel_size=3, padding=(1,1))\n",
    "        \n",
    "        self.activation = nn.ReLU() # \"each followed by a ReLU\"\n",
    " \n",
    "    def forward(self, x, skip_con_x):\n",
    "        '''\n",
    "        Function for completing a forward pass of ExpandingBlock: \n",
    "        Given an image tensor, completes an expanding block and returns the transformed tensor.\n",
    "        Parameters:\n",
    "            x: image tensor of shape (batch size, channels, height, width)\n",
    "            skip_con_x: the image tensor from the contracting path (from the opposing block of x)\n",
    "                    for the skip connection\n",
    "                    \n",
    "        Note: In the original Unet implementation, the output shape is smaller than the input, which \n",
    "        requires a skip connection layer size to be matched with current layer. \n",
    "        In this application, since our input and output are to be same size, we will note crop the \n",
    "        skip connection layer. However, there is a placehold commented, if needed in future. \n",
    "        \n",
    "        '''\n",
    "        x = self.upsample(x)\n",
    "        x = self.conv1(x)        \n",
    "        x = torch.cat([x, skip_con_x], axis=1)\n",
    "        x = self.conv2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9012c9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureMapBlock(nn.Module):\n",
    "    '''\n",
    "    FeatureMapBlock Class\n",
    "    The final layer of a UNet - \n",
    "    maps each pixel to a pixel with the correct number of output dimensions\n",
    "    using a 1x1 convolution.\n",
    "    Values:\n",
    "        input_channels: the number of channels to expect from a given input\n",
    "    '''\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super(FeatureMapBlock, self).__init__()\n",
    "        \n",
    "        # \"Every step in the expanding path consists of an upsampling of the feature map\"\n",
    "        self.conv = nn.Conv2d(input_channels, output_channels, kernel_size=1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Function for completing a forward pass of FeatureMapBlock: \n",
    "        Given an image tensor, returns it mapped to the desired number of channels.\n",
    "        Parameters:\n",
    "            x: image tensor of shape (batch size, channels, height, width)\n",
    "        '''\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd88b349",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    '''\n",
    "    UNet Class\n",
    "    A series of 4 contracting blocks followed by 4 expanding blocks to \n",
    "    transform an input image into the corresponding paired image, with an upfeature\n",
    "    layer at the start and a downfeature layer at the end\n",
    "    Values:\n",
    "        input_channels: the number of channels to expect from a given input\n",
    "        output_channels: the number of channels to expect for a given output\n",
    "    '''\n",
    "    def __init__(self, input_channels, output_channels, hidden_channels=64):\n",
    "        super(UNet, self).__init__()        \n",
    "        # \"Every step in the expanding path consists of an upsampling of the feature map\"\n",
    "        self.upfeature = FeatureMapBlock(input_channels, hidden_channels)\n",
    "        self.contract1 = ContractingBlock(hidden_channels)\n",
    "        self.contract2 = ContractingBlock(hidden_channels * 2)\n",
    "        self.contract3 = ContractingBlock(hidden_channels * 4)\n",
    "        self.contract4 = ContractingBlock(hidden_channels * 8)\n",
    "        self.expand1 = ExpandingBlock(hidden_channels * 16)\n",
    "        self.expand2 = ExpandingBlock(hidden_channels * 8)\n",
    "        self.expand3 = ExpandingBlock(hidden_channels * 4)\n",
    "        self.expand4 = ExpandingBlock(hidden_channels * 2)\n",
    "        self.downfeature = FeatureMapBlock(hidden_channels, output_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Function for completing a forward pass of UNet: \n",
    "        Given an image tensor, passes it through U-Net and returns the output.\n",
    "        Parameters:\n",
    "            x: image tensor of shape (batch size, channels, height, width)\n",
    "        '''\n",
    "        # Keep in mind that the expand function takes two inputs, \n",
    "        # both with the same number of channels.                 \n",
    "        x0 = self.upfeature(x)\n",
    "        x1 = self.contract1(x0)        \n",
    "        x2 = self.contract2(x1)        \n",
    "        x3 = self.contract3(x2)\n",
    "        x4 = self.contract4(x3)\n",
    "        \n",
    "        x5 = self.expand1(x4, x3)\n",
    "        x6 = self.expand2(x5, x2)\n",
    "        x7 = self.expand3(x6, x1)\n",
    "        x8 = self.expand4(x7, x0)\n",
    "        xn = self.downfeature(x8)          \n",
    "        return xn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003e485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to(image, new_shape):\n",
    "    '''\n",
    "    Function for padding an image tensor. \n",
    "    If somehow the expanding layer output and the skip connection doesn't match,\n",
    "    these might be helpful.\n",
    "    '''\n",
    "    h, w = image.shape[0], image.shape[1]\n",
    "    new_h, new_w = new_shape[0], new_shape[1]\n",
    "    \n",
    "    inc_h, inc_w = new_h -h, new_w - w\n",
    "    left, right = 0, inc_w\n",
    "    top, bottom = 0, inc_h\n",
    "    pads = left, right, top, bottom \n",
    "    \n",
    "    # zero-padding by default.\n",
    "    # See others at https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.pad\n",
    "    padded_image = F.pad(image, pads, \"constant\", 0)\n",
    "\n",
    "    return padded_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc7f96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special function to display images side by side after training\n",
    "\n",
    "def show_tensor_images(image, fault, pred, num_images=25, size=(1, 28, 28)):\n",
    "    '''\n",
    "    Function for visualizing images: Given a tensor of images, number of images, and\n",
    "    size per image, plots and prints the images in an uniform grid.\n",
    "    '''\n",
    "    # image_shifted = (image_tensor + 1) / 2\n",
    "    #image_shifted = image_tensor\n",
    "    \n",
    "    image_unflat = image.detach().cpu()\n",
    "    fault_unflat = fault.detach().cpu()\n",
    "    pred_unflat = pred.detach().cpu()\n",
    "    \n",
    "    \n",
    "    fig = plt.figure(figsize=(12,15))\n",
    "    \n",
    "    ax = fig.add_subplot(331)    \n",
    "    ax.imshow(image_unflat.squeeze(), cmap = 'gray')\n",
    "    ax.set_title(\"Seismic Image\")\n",
    "    \n",
    "    ax = fig.add_subplot(332)\n",
    "    ax.imshow(image_unflat.squeeze(), cmap = 'gray')\n",
    "    ax.imshow(fault_unflat.squeeze(), cmap = 'gray', vmin=0, vmax=1, alpha=0.4)\n",
    "    ax.set_title(\"Fault\")\n",
    "    \n",
    "    ax = fig.add_subplot(333)\n",
    "    ax.imshow(image_unflat.squeeze(), cmap = 'gray')\n",
    "    ax.imshow(pred_unflat.squeeze(), cmap = 'gray', vmin=0, vmax=1, alpha=0.4)\n",
    "    ax.set_title(\"Predicted Fault\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5845f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4421fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "n_epochs = 3    \n",
    "input_dim = 1\n",
    "label_dim = 1\n",
    "display_step = 1397\n",
    "batch_size = 1\n",
    "lr = 0.0005\n",
    "initial_shape = 512\n",
    "target_shape = 512\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351414e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid():\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True)\n",
    "    valid_dataloader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size = batch_size,\n",
    "        shuffle=False)\n",
    "    unet = UNet(input_dim, label_dim).to(device)\n",
    "    unet_opt = torch.optim.Adam(unet.parameters(), lr=lr)\n",
    "    cur_step = 0\n",
    "    \n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        loss_iter = []\n",
    "                \n",
    "        for real, labels in tqdm(train_dataloader):\n",
    "            \n",
    "            cur_batch_size = len(real)\n",
    "            # Flatten the image\n",
    "            real = real.to(device)\n",
    "            labels = labels.to(device)            \n",
    "                      \n",
    "\n",
    "            ### Update U-Net ###\n",
    "            unet_opt.zero_grad()\n",
    "            pred = unet(real)\n",
    "            #print(pred.shape)\n",
    "            unet_loss = criterion(pred, labels)\n",
    "            unet_loss.backward()\n",
    "            unet_opt.step()\n",
    "            \n",
    "            loss_iter.append(unet_loss.item())\n",
    "            \n",
    "\n",
    "            if cur_step % display_step == 0:\n",
    "                print(f\"Epoch {epoch}: Step {cur_step}: Training loss: {unet_loss.item()}\")\n",
    "                #show_tensor_images(\n",
    "                    #crop(real, torch.Size([len(real), 1, target_shape, target_shape])), \n",
    "                    #size=(input_dim, target_shape, target_shape)\n",
    "                #)\n",
    "                #show_tensor_images(real.T, size=(input_dim, target_shape, target_shape))                \n",
    "                #show_tensor_images(labels.T, size=(label_dim, target_shape, target_shape))\n",
    "                #show_tensor_images(torch.sigmoid(pred).T, size=(label_dim, target_shape, target_shape))\n",
    "                print('Training:')\n",
    "                show_tensor_images(real.T, labels.T, torch.sigmoid(pred).T, size=(input_dim, target_shape, target_shape))\n",
    "            cur_step += 1\n",
    "        \n",
    "        train_losses.append(statistics.mean(loss_iter))\n",
    "        print(f\"Training loss in all completed epoch: {train_losses}\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for val_input, val_labels in tqdm(valid_dataloader):\n",
    "                val_input = val_input.to(device)\n",
    "                val_labels = val_labels.to(device)\n",
    "                val_outputs = unet(val_input)\n",
    "                val_loss = criterion(val_outputs, val_labels)\n",
    "        print('Validation:')\n",
    "        print(f\"Validation loss in {epoch} epoch: {val_loss.item()}\")\n",
    "        show_tensor_images(val_input.T, val_labels.T, torch.sigmoid(val_outputs).T, size=(input_dim, target_shape, target_shape))\n",
    "        \n",
    "        valid_losses.append(val_loss)       \n",
    "        \n",
    "    return unet, pred, train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a63b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, pred, train_loss, valid_loss = train_valid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce334622",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_T = torch.Tensor(train_loss)\n",
    "valid_loss_T = torch.Tensor(valid_loss)\n",
    "train_loss_cpu = train_loss_T.cpu().detach()\n",
    "valid_loss_cpu = valid_loss_T.cpu().detach()\n",
    "fig, ax = plt.subplots(1, figsize=(8,6))\n",
    "ax.plot(train_loss_cpu, color = 'red', label = 'training loss')\n",
    "ax.plot(valid_loss_cpu, color = 'blue', label = 'validation loss')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.xticks(range(0,19))\n",
    "plt.legend(loc='upper right', frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8fcae6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
